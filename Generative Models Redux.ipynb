{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrJwIlKpnHgH"
   },
   "source": [
    "### Group members (first and last names):\n",
    "- #### *Abdelhak Kermia*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnXg_AaRnHgJ"
   },
   "source": [
    "# Introduction\n",
    "In this notebook, youâ€™ll dive into the world of Recurrent Neural Networks (RNNs) by implementing a simple Elman RNN from scratch and comparing it to PyTorchâ€™s built-in RNN.\n",
    "\n",
    "Youâ€™ll learn how sequence models handle character predictions and how sampling affects output quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-lav9hnnHgL"
   },
   "source": [
    "---\n",
    "## How to pass the assignment?\n",
    "Below, you will find the exercise questions. Each question that awards points is numbered and displays the available points in this format: **(0 pts)**.\n",
    "\n",
    "### Answering Questions\n",
    "- Provide your answers in the cell directly below each question.\n",
    "- Use **Markdown** for text-based answers (in **English**).\n",
    "- Use **code cells** for implementations.\n",
    "\n",
    "### Critical Thinking Questions and Bonus Exercises\n",
    "- Some questions are marked with a ðŸ§  (Critical Thinking) or a â­ (Bonus Exercise). These are for self-reflection and extra practice.\n",
    "- They are **optional** and do **not** award any points.\n",
    "- Answering them can help reinforce your understanding.\n",
    "\n",
    "### Important Rules\n",
    "- Only use the Python packages introduced in the assignment. Using unauthorized packages will result in **0 points** for the affected question.\n",
    "- Follow dataset instructions carefully.\n",
    "  - If no new dataset is mentioned, continue using the one from the previous task.\n",
    "  - Using a different dataset than instructed will result in **0 points** for that question.\n",
    "- All code must run correctly.\n",
    "  - If your code does not execute, you will receive a **50% deduction** for that question.\n",
    "  - Always test your code before submitting.\n",
    "- Incorrect or incomplete answers receive **0 points**.\n",
    "  - Partial credit may be awarded if the core idea is correct **and** the instructions are followed precisely.\n",
    "  - If you do not follow the instructions, you will receive **0 points**, regardless of effort or length.\n",
    "- Do not provide overly detailed or off-topic answers. Stay focused on what is asked. Extra information does not earn extra points.\n",
    "\n",
    "### Important Notes\n",
    "- Save your work frequently! (Ctrl + S)\n",
    "- Before submitting, `Restart Session and Run All` cells to ensure everything works correctly.\n",
    "- **You need at least 17 points out of 25 (66%) to pass âœ…**\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:10.172631Z",
     "start_time": "2025-05-18T12:51:10.169116Z"
    },
    "id": "HRSaGH8rFLSv"
   },
   "outputs": [],
   "source": [
    "points = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "blyAu-4ZlAGi"
   },
   "source": [
    "# 1. Fundamentals (4 points)\n",
    "Before diving into code, let's test your understanding of **?????** with these questions. For each topic, identify which statements are TRUE âœ… and which are FALSE âŒ. Each question may have 1, 2, 3, or 4 correct answers.\n",
    "\n",
    "- 4 correct answers: 2 points\n",
    "- 3 correct answers: 1 point\n",
    "- 2 or fewer correct answers: 0 points\n",
    "\n",
    "ðŸ’¡ In Google Colab, you can easily add emojis to markdown cells by typing `:` followed by the emoji's name. For example, typing `:light-bulb` will display a light bulb emoji. This feature is also available as an extension in many IDEs.\n",
    "\n",
    "â— **TIP:** If a term is unfamiliar to you, look it up in [Google's ML Glossary](https://developers.google.com/machine-learning/glossary) for a simple explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G_pCcJjelAGj"
   },
   "source": [
    "#### 1.1 **(2pts) Which of the following statements about Recurrent Neural Networks (RNNs) are correct?**\n",
    "\n",
    " A. RNNs are particularly suited for sequential data like language âœ…\n",
    "\n",
    " B. RNNs can update internal representations based on previously processed inputs âœ…\n",
    "\n",
    " C. RNNs require the input to be passed through a convolutional layer before processing âŒ\n",
    "\n",
    " D. The hidden state $h_t$ in RNNs is updated based on  $x_t$ and $h_{t-1}$ âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuF9Qagje-sy"
   },
   "source": [
    "#### 1.2 **(2pts) Which of the following describe layers or transformations used when *training* a basic RNN?**\n",
    "\n",
    " A. $h_t = \\text{tahn}(W_{ih}x_t + W_{hh}h_{t-1} + b_h)$ âœ…\n",
    "\n",
    " B. $o_t = W_{ho}h_t + b_o$ âœ…\n",
    "\n",
    " C. $\\hat{y} = \\text{softmax}(o_t)$ âœ…\n",
    "\n",
    " D. $\\mathscr{L} = -\\text{log}\\hat{y_t}[y_t]$ âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HXVEfcrrlAGj"
   },
   "source": [
    "# 2. Hands-On (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8JlVeZHlAGk"
   },
   "source": [
    "Suppose you are working with the following character vocabulary for a character-level RNN:\n",
    "\n",
    "`vocab = ['a', 'c', 'd', 'e', 'o', 'r']`\n",
    "\n",
    "The characters are mapped to integers in the order shown:\n",
    "\n",
    "| Character | Index |\n",
    "| --------- | ----- |\n",
    "| 'a'       | 0     |\n",
    "| 'c'       | 1     |\n",
    "| 'd'       | 2     |\n",
    "| 'e'       | 3     |\n",
    "| 'o'       | 4     |\n",
    "| 'r'       | 5     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZNItMk5olAGl"
   },
   "source": [
    "#### 2.1 **(1pt) What is the one-hot encoded vector for the character 'e'?**\n",
    "Write your answer in LaTeX as a column vector.\n",
    "\n",
    "Example format (for character `'a'`):\n",
    "\n",
    "$$ \\text{one-hot}(\\texttt{a}) =\n",
    "\\begin{bmatrix}\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyp-HPYblAGl"
   },
   "source": [
    "- for the character e:\n",
    "$$ \\text{one-hot}(\\texttt{e}) =\n",
    "\\begin{bmatrix}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1 \\\\\n",
    "0 \\\\\n",
    "0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YIMBG4CzlAGl"
   },
   "source": [
    "#### 2.2 **(1pt) Decode the following sequence using the vocabulary above and write the resulting word.**\n",
    "\n",
    "`encoded = [2, 3, 1, 4, 2, 3, 5]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLTnqGuqlAGn"
   },
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "toIF_M1glAGn"
   },
   "source": [
    "#### 2.3 **(1pt) Construct the one-hot encoded matrix $X \\in \\mathbb{R}^{6 \\times 7}$ for this sequence using LaTeX.**\n",
    "\n",
    "Example format for an empty 6x7 matrix:\n",
    "\n",
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4qtHYV-lAGn"
   },
   "source": [
    "Each column in the matrix represents a one-hot encoded character vector.\n",
    "$$ X =\n",
    "\\begin{bmatrix}\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
    "1 & 0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & 0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWjVwR6ylAGo"
   },
   "source": [
    "# 3. Coding (12 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.012173Z",
     "start_time": "2025-05-18T12:51:10.524917Z"
    },
    "id": "dsRlXDr6lAGp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# These are the packages you'll need today\n",
    "# If you're running on a local environment, make sure everything you need is installed :)\n",
    "\n",
    "# Data manipulation and visualization\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# PyTorch libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader # Redundant\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Set random seed\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(0)\n",
    "\n",
    "# Set up device (GPU if available, else CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MJ-mZS9lAGq"
   },
   "source": [
    "## Task: Character-Level Text Generation with RNNs\n",
    "In this assignment, your goal is to generate Shakespearean-style text by training a model to predict the next character in a sequence. You will approach this task using different types of recurrent neural networks (RNNs).\n",
    "\n",
    "> *â€œTo RNN or not to RNN, that is the question.â€* ðŸ¤–ðŸŽ­\n",
    "\n",
    "ðŸŽ¯ Objectives\n",
    "1. **Text Generation from Characters:**\n",
    "    - You'll train a character-level model to learn the patterns of Shakespeare's writing. Given a sequence of characters, the model will try to predict the next character in the sequence, enabling it to generate realistic (or amusing!) text letter by letter.\n",
    "\n",
    "2. **Implement a Custom Elman RNN:**\n",
    "    - You will implement your own version of a simple RNN from scratch (also known as an Elman network). This involves manually managing hidden states and performing step-by-step sequence processing without using PyTorch's built-in `nn.RNN` class.\n",
    "\n",
    "3. **Compare with Built-in PyTorch Models:**\n",
    "    - Youâ€™ll then train and evaluate a PyTorch-based RNN using `nn.RNN`\n",
    "    - You will compare the text these models generate and discuss how the model architecture affects the quality of the output (e.g., coherence, structure, creativity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8xHqr3K3lAGq"
   },
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.035961Z",
     "start_time": "2025-05-18T12:51:13.026598Z"
    },
    "id": "7uM-167hlAGr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load shakespeare text and save as text\n",
    "with open(\"shakespeare.txt\", \"r\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Print the length of the text\n",
    "print(f\"Length of text: {len(text)} characters\")\n",
    "\n",
    "# Show the first 100 characters of the text\n",
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.125974Z",
     "start_time": "2025-05-18T12:51:13.121632Z"
    },
    "id": "Jb59GD4SlAGr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique characters: 77\n"
     ]
    }
   ],
   "source": [
    "# Set valid characters for the model to generate\n",
    "chars = ['\\n', ' ', '!', '\"', '$', '&', \"'\", ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "\n",
    "# Print the number of unique characters\n",
    "print(f\"Number of unique characters: {len(chars)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PlJ1KqtKlAGs"
   },
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.239069Z",
     "start_time": "2025-05-18T12:51:13.234356Z"
    },
    "id": "FRVmRGMplAGs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '$': 4, '&': 5, \"'\": 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '>': 22, '?': 23, 'A': 24, 'B': 25, 'C': 26, 'D': 27, 'E': 28, 'F': 29, 'G': 30, 'H': 31, 'I': 32, 'J': 33, 'K': 34, 'L': 35, 'M': 36, 'N': 37, 'O': 38, 'P': 39, 'Q': 40, 'R': 41, 'S': 42, 'T': 43, 'U': 44, 'V': 45, 'W': 46, 'X': 47, 'Y': 48, 'Z': 49, '`': 50, 'a': 51, 'b': 52, 'c': 53, 'd': 54, 'e': 55, 'f': 56, 'g': 57, 'h': 58, 'i': 59, 'j': 60, 'k': 61, 'l': 62, 'm': 63, 'n': 64, 'o': 65, 'p': 66, 'q': 67, 'r': 68, 's': 69, 't': 70, 'u': 71, 'v': 72, 'w': 73, 'x': 74, 'y': 75, 'z': 76}\n"
     ]
    }
   ],
   "source": [
    "# Map characters to integers and vice versa\n",
    "char_to_int = {ch: i for i, ch in enumerate(chars)}\n",
    "int_to_char = {i: ch for ch, i in char_to_int.items()}\n",
    "\n",
    "# Print the mapping of characters to integers\n",
    "print(char_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.523704Z",
     "start_time": "2025-05-18T12:51:13.374703Z"
    },
    "id": "kNx42kf3lAGt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29, 59, 68, 69, 70,  1, 26, 59, 70, 59, 76, 55, 64, 20,  0, 25, 55,\n",
       "       56, 65, 68, 55,  1, 73, 55,  1, 66, 68, 65, 53, 55, 55, 54,  1, 51,\n",
       "       64, 75,  1, 56, 71, 68, 70, 58, 55, 68,  7,  1, 58, 55, 51, 68,  1,\n",
       "       63, 55,  1, 69, 66, 55, 51, 61,  9,  0,  0, 24, 62, 62, 20,  0, 42,\n",
       "       66, 55, 51, 61,  7,  1, 69, 66, 55, 51, 61,  9,  0,  0, 29, 59, 68,\n",
       "       69, 70,  1, 26, 59, 70, 59, 76, 55, 64, 20,  0, 48, 65, 71])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Encode the shakespearean text to integers\n",
    "encoded = np.array([char_to_int[ch] for ch in text])\n",
    "\n",
    "# Print the first 100 encoded characters\n",
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gjyS-QLlAGt"
   },
   "source": [
    "## One-Hot Encoding\n",
    "This function converts a 2D tensor of integer character indices into one-hot encoded vectors.\n",
    "\n",
    "**Input:**\n",
    "- `arr`: A 2D tensor of shape (batch_size, seq_length)\n",
    "Each element is an integer index corresponding to a character in the vocabulary.\n",
    "- `n_labels`: The size of the vocabulary (i.e., number of unique characters).\n",
    "\n",
    "**Output:**\n",
    "- A 3D tensor of shape (batch_size, seq_length, n_labels) where each character index is replaced by a one-hot vector.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "If `arr = [[1, 0], [2, 3]]` and `n_labels = 4`, the output will be:\n",
    "```\n",
    "[\n",
    " [[0, 1, 0, 0], [1, 0, 0, 0]],\n",
    " [[0, 0, 1, 0], [0, 0, 0, 1]]\n",
    "]\n",
    "```\n",
    "**ðŸ§  Do you know what the shape of this tensor is?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.548263Z",
     "start_time": "2025-05-18T12:51:13.543846Z"
    },
    "id": "TULqscBXlAGu"
   },
   "outputs": [],
   "source": [
    "# Define method to encode one hot labels\n",
    "##### arr (batch x seq_length) ----> (batch x seq_length x vocabulary_size)\n",
    "def one_hot_encode(arr, n_labels):\n",
    "\n",
    "    # Initialize the the encoded array\n",
    "    one_hot = torch.zeros(list(arr.shape) + [n_labels], dtype=torch.float32)\n",
    "    one_hot = one_hot.view(arr.shape[0] * arr.shape[1], -1)\n",
    "\n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[torch.arange(one_hot.shape[0]), arr.view(-1)] = 1.\n",
    "\n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_NDGO5PlAGv"
   },
   "source": [
    "## Custom Dataset for RNN Training\n",
    "To train an RNN to predict the next character in a sequence, we need to prepare our data in a way that provides both **input sequences** and **target sequences**. The `SubsequencesDataset` class helps with this by slicing a long 1D sequence of encoded data into multiple **overlapping** sub-sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.614651Z",
     "start_time": "2025-05-18T12:51:13.610102Z"
    },
    "id": "9BsgXdHQlAGv"
   },
   "outputs": [],
   "source": [
    "class SubsequencesDataset(Dataset):\n",
    "    def __init__(self, data: np.ndarray, seq_length: int):\n",
    "        super(SubsequencesDataset, self).__init__()\n",
    "\n",
    "        self.data = data # Full 1D array of encoded characters\n",
    "        self.seq_length = seq_length # Length of input sequences\n",
    "\n",
    "    def __len__(self):\n",
    "        # Determines the number of full (input, target) sequences can be extracted\n",
    "        if self.data.shape[0] % self.seq_length == 0:\n",
    "            return self.data.shape[0] // self.seq_length - 1\n",
    "        else:\n",
    "            return self.data.shape[0] // self.seq_length\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        # Extracts a single (input, target) sequences\n",
    "        return (self.data[index * self.seq_length:index * self.seq_length + self.seq_length], # Input sequence\n",
    "                self.data[index * self.seq_length + 1:index * self.seq_length + self.seq_length + 1]) # Target sequence (input shifted by 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvxmP90BlAGw"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.696390Z",
     "start_time": "2025-05-18T12:51:13.692295Z"
    },
    "id": "wdSuJC6RlAGw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset: 2\n",
      "First input, target sequence: (array([2, 1, 4, 7]), array([1, 4, 7, 0]))\n",
      "Second input, target sequence: (array([ 0, 23, 57, 12]), array([23, 57, 12, 11]))\n"
     ]
    }
   ],
   "source": [
    "dataset = SubsequencesDataset(data=np.array([2, 1, 4, 7, 0, 23, 57, 12, 11, 8]), seq_length=4)\n",
    "print(f\"Length of dataset: {len(dataset)}\")\n",
    "print(f\"First input, target sequence: {dataset[0]}\")\n",
    "print(f\"Second input, target sequence: {dataset[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 **(2pts) Implement an Elman RNN from scratch.**\n",
    "Below is the skeleton of an Elman RNN. Your task is to complete three lines in the `__init__` method and one line in the `forward` method.\n",
    "\n",
    "1. In `__init__`, initialize the following layers using [`torch.nn.Linear`](https://docs.pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear):\n",
    "    - `self.W_ih`: input-to-hidden layer\n",
    "    - `self.W_hh`: hidden-to-hidden layer (without a bias term)\n",
    "    - `self.out`: hidden-to-output layer\n",
    "\n",
    "2. In `forward`, update the hidden state by computing the new value of the variable `h` using the formula provided in the lecture notes.\n",
    "    - You may use the appropriate non-linear activation function from [`torch.nn.functional`](https://docs.pytorch.org/docs/stable/nn.functional.html#non-linear-activation-functions), imported as `F`.\n",
    "    - Make sure to omit the bias term because we initalized `self.W_hh` without one ðŸ˜‰.\n",
    "\n",
    "Use only `torch.nn` and `torch.nn.functional`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.804017Z",
     "start_time": "2025-05-18T12:51:13.798961Z"
    }
   },
   "outputs": [],
   "source": [
    "class ElmanRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.W_ih = nn.Linear(input_size, hidden_size)\n",
    "        self.W_hh = nn.Linear(hidden_size, hidden_size, bias=False)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        ######################\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        h = hidden\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]\n",
    "            ### YOUR CODE HERE ###\n",
    "            h = F.tanh(self.W_ih(x_t) + self.W_hh(h))\n",
    "            # Do we need to remove bias b_ih  from W_ih too here (b_h = b_ih + b_hh) ? F.tanh(F.linear(x_t, self.W_ih.weight)  self.W_hh(h)) remove the b_ih bias from W_ih bypassing initialization. Why don't we do it before in init? Is it to use torch.functional in the forward to see a bypass ?\n",
    "            ######################\n",
    "            outputs.append(self.out(h))\n",
    "\n",
    "        out = torch.stack(outputs, dim=1)\n",
    "        out = out.contiguous().view(batch_size * seq_len, -1)\n",
    "        return out, h\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.W_hh.in_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation, as it is now is fine. We don't need to remove the bias from `self.W_ih`, and we also don't have to add a bias explicitely in the activation since we already have one in `self.W_ih`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SGwjRPLHlAGw"
   },
   "source": [
    "#### 3.2 **(2pts) Which of the following statements about the ElmanRNN class are TRUE?**\n",
    "*The same multiple-answer rules apply from the fundamentals section, meaning there could be one or more correct answers.*\n",
    "\n",
    " A. The model supports multi-layer RNNs using `self.W_hh`.âŒ\n",
    "\n",
    " B. The model manually computes the hidden state at each timestep using `torch.tanh(...)`. âœ…\n",
    "\n",
    " C. `self.W_ih` and `self.W_hh` are used together to update the hidden state. âœ…\n",
    "\n",
    " D. `init_hidden()` creates a hidden state tensor of shape (batch_size, hidden_size). âœ…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yZRZILaOe-tF"
   },
   "source": [
    "> *Remember thee? \\\n",
    "Yea, from the table of my memory \\\n",
    "I'll wipe away all trivial fond records, \\\n",
    "All saws of books, all forms, all pressures past\"*\n",
    "\n",
    "\\- Hamlet Act 1, Scene 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 **(1pt) Write an RNN with `nn.RNN`.**\n",
    "Below is the skeleton of a vanilla RNN model for character-level text generation. Your task is to complete only two lines inside the `__init__` method.\n",
    "1. Create the RNN layer and assign it to `self.rnn`\n",
    "    - Use `nn.RNN` from PyTorch\n",
    "    \n",
    "    Parameters:\n",
    "    - `input_size` (number of input features)\n",
    "    - `hidden_size` (number of features in the hidden state)\n",
    "    - `num_layers` (how many stacked RNN layers)\n",
    "    - `batch_first=True` (ensures input shape is (batch, seq, feature))\n",
    "    - `dropout=0.5` (applies dropout between RNN layers)\n",
    "\n",
    "2. Create the output layer and assign it to `self.fc`\n",
    "    - Use `nn.Linear` to map from `hidden_size` to `output_size`\n",
    "\n",
    "You can use: `torch`, `torch.nn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.885407Z",
     "start_time": "2025-05-18T12:51:13.880448Z"
    },
    "id": "hf-nTEDRlAGx"
   },
   "outputs": [],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.output_size = output_size\n",
    "\n",
    "        ### YOUR CODE HERE ###\n",
    "        self.rnn = nn.RNN(self.input_size, self.hidden_size, num_layers=self.num_layers, batch_first=True, dropout=0.5)\n",
    "        self.fc= nn.Linear(self.hidden_size, self.output_size)\n",
    "        ######################\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = self.fc(out.contiguous().view(-1, out.size(2)))\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(self.num_layers, batch_size, self.rnn.hidden_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TMRiDMplAGy"
   },
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:13.964Z",
     "start_time": "2025-05-18T12:51:13.955599Z"
    },
    "id": "pysqXyAPlAGy"
   },
   "outputs": [],
   "source": [
    "# This function was adapted from Deep Learning by Prof. Paolo Favaro, University of Bern\n",
    "\n",
    "def train(model, data, vocab_size, epochs=20, batch_size=128, seq_length=100, lr=0.001, clip=5, val_frac=0.1, print_every=1, device=device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Create train/val splits\n",
    "    dataset = SubsequencesDataset(data, seq_length=seq_length)\n",
    "    train_size = int(len(dataset) * (1 - val_frac))\n",
    "    val_size = len(dataset) - train_size\n",
    "    training_set, validation_set = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(training_set, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    val_loader = DataLoader(validation_set, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "    print(f\"Training {model.__class__.__name__} on {device}\")\n",
    "\n",
    "    for e in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            h = model.init_hidden(x.size(0))\n",
    "            h = tuple(h_.to(device) for h_ in h) if isinstance(h, tuple) else h.to(device)\n",
    "\n",
    "            x = one_hot_encode(x, vocab_size).to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            model.zero_grad()\n",
    "            output, h = model(x, h)\n",
    "            loss = criterion(output, y.view(-1).long())\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "            opt.step()\n",
    "\n",
    "        if e % print_every == 0:\n",
    "            model.eval()\n",
    "            val_losses = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x_val, y_val in val_loader:\n",
    "                    val_h = model.init_hidden(x_val.size(0))\n",
    "                    val_h = tuple(vh.to(device) for vh in val_h) if isinstance(val_h, tuple) else val_h.to(device)\n",
    "\n",
    "                    x_val = one_hot_encode(x_val, vocab_size).to(device)\n",
    "                    y_val = y_val.to(device)\n",
    "\n",
    "                    val_out, val_h = model(x_val, val_h)\n",
    "                    val_loss = criterion(val_out, y_val.view(-1).long())\n",
    "                    val_losses.append(val_loss.item())\n",
    "\n",
    "            model.train()\n",
    "            print(f\"Epoch: {e+1}/{epochs}... \"\n",
    "                    f\"Loss: {loss.item():.4f}... Val Loss: {np.mean(val_losses):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aB5vXEYNlAGz"
   },
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:51:14.053346Z",
     "start_time": "2025-05-18T12:51:14.049920Z"
    },
    "id": "dGklt7_qlAGz"
   },
   "outputs": [],
   "source": [
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "\n",
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20\n",
    "\n",
    "vocab_size = len(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 **(2pts) Initialize and train the 2 models (ElmanRNN, VanillaRNN).**\n",
    "- Use the hyperparameters and `train` function defined above.\n",
    "\n",
    "âŒ› Each model will take between 30 seconds - 3 minutes to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:53:25.392846Z",
     "start_time": "2025-05-18T12:51:14.128673Z"
    },
    "id": "0mXnCoMUlAG2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elman RNN Training:\n",
      "Training ElmanRNN on cuda\n",
      "Epoch: 1/20... Loss: 3.1049... Val Loss: 3.0896\n",
      "Epoch: 2/20... Loss: 2.5488... Val Loss: 2.5619\n",
      "Epoch: 3/20... Loss: 2.3679... Val Loss: 2.3475\n",
      "Epoch: 4/20... Loss: 2.2021... Val Loss: 2.2355\n",
      "Epoch: 5/20... Loss: 2.1751... Val Loss: 2.1644\n",
      "Epoch: 6/20... Loss: 2.1061... Val Loss: 2.1031\n",
      "Epoch: 7/20... Loss: 2.0626... Val Loss: 2.0510\n",
      "Epoch: 8/20... Loss: 2.0469... Val Loss: 2.0113\n",
      "Epoch: 9/20... Loss: 1.9748... Val Loss: 1.9724\n",
      "Epoch: 10/20... Loss: 1.9382... Val Loss: 1.9457\n",
      "Epoch: 11/20... Loss: 1.9440... Val Loss: 1.9131\n",
      "Epoch: 12/20... Loss: 1.8963... Val Loss: 1.8905\n",
      "Epoch: 13/20... Loss: 1.8701... Val Loss: 1.8590\n",
      "Epoch: 14/20... Loss: 1.7796... Val Loss: 1.8355\n",
      "Epoch: 15/20... Loss: 1.8059... Val Loss: 1.8159\n",
      "Epoch: 16/20... Loss: 1.7750... Val Loss: 1.8005\n",
      "Epoch: 17/20... Loss: 1.7696... Val Loss: 1.7787\n",
      "Epoch: 18/20... Loss: 1.7672... Val Loss: 1.7773\n",
      "Epoch: 19/20... Loss: 1.7816... Val Loss: 1.7473\n",
      "Epoch: 20/20... Loss: 1.7261... Val Loss: 1.7391\n",
      "Vanilla RNN Training:\n",
      "Training VanillaRNN on cuda\n",
      "Epoch: 1/20... Loss: 3.2424... Val Loss: 3.2276\n",
      "Epoch: 2/20... Loss: 2.4354... Val Loss: 2.4326\n",
      "Epoch: 3/20... Loss: 2.2700... Val Loss: 2.2236\n",
      "Epoch: 4/20... Loss: 2.0906... Val Loss: 2.0980\n",
      "Epoch: 5/20... Loss: 2.0276... Val Loss: 2.0014\n",
      "Epoch: 6/20... Loss: 1.9506... Val Loss: 1.9299\n",
      "Epoch: 7/20... Loss: 1.9324... Val Loss: 1.8709\n",
      "Epoch: 8/20... Loss: 1.8769... Val Loss: 1.8243\n",
      "Epoch: 9/20... Loss: 1.8220... Val Loss: 1.7851\n",
      "Epoch: 10/20... Loss: 1.7662... Val Loss: 1.7549\n",
      "Epoch: 11/20... Loss: 1.7339... Val Loss: 1.7279\n",
      "Epoch: 12/20... Loss: 1.7265... Val Loss: 1.7061\n",
      "Epoch: 13/20... Loss: 1.6927... Val Loss: 1.6889\n",
      "Epoch: 14/20... Loss: 1.7198... Val Loss: 1.6719\n",
      "Epoch: 15/20... Loss: 1.6262... Val Loss: 1.6489\n",
      "Epoch: 16/20... Loss: 1.6341... Val Loss: 1.6371\n",
      "Epoch: 17/20... Loss: 1.7011... Val Loss: 1.6247\n",
      "Epoch: 18/20... Loss: 1.6127... Val Loss: 1.6157\n",
      "Epoch: 19/20... Loss: 1.6433... Val Loss: 1.6011\n",
      "Epoch: 20/20... Loss: 1.6001... Val Loss: 1.5968\n"
     ]
    }
   ],
   "source": [
    "Elman_RNN = ElmanRNN(input_size=vocab_size,hidden_size=hidden_size,output_size=vocab_size)\n",
    "print(\"Elman RNN Training:\")\n",
    "train(model = Elman_RNN,\n",
    "      data = encoded,\n",
    "      vocab_size = vocab_size,\n",
    "      epochs = n_epochs,\n",
    "      batch_size = batch_size,\n",
    "      seq_length = seq_length,\n",
    "      )\n",
    "\n",
    "print(\"Vanilla RNN Training:\")\n",
    "Vanilla_RNN = VanillaRNN(input_size=vocab_size,hidden_size=hidden_size,output_size=vocab_size,num_layers=num_layers)\n",
    "train(model = Vanilla_RNN,\n",
    "      data = encoded,\n",
    "      vocab_size = vocab_size,\n",
    "      epochs = n_epochs,\n",
    "      batch_size = batch_size,\n",
    "      seq_length = seq_length\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OzDoZ4w1lAG4"
   },
   "source": [
    "## Prediction and Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:53:25.422069Z",
     "start_time": "2025-05-18T12:53:25.417350Z"
    },
    "id": "SxsRPpAplAG4"
   },
   "outputs": [],
   "source": [
    "# This function was adapted from Deep Learning by Prof. Paolo Favaro, University of Bern\n",
    "\n",
    "def predict(model, char, h=None, top_k=None, device=device):\n",
    "    ''' Given a character, predict the next character.\n",
    "        Returns the predicted character and the hidden state.\n",
    "    '''\n",
    "\n",
    "    # Char to int â†’ to one-hot â†’ to device\n",
    "    x = torch.LongTensor([[char_to_int[char]]])\n",
    "    x = one_hot_encode(x, vocab_size).to(device)\n",
    "\n",
    "    # Move hidden state to same device\n",
    "    if isinstance(h, tuple):\n",
    "        h = tuple(each.to(device) for each in h)\n",
    "    else:\n",
    "        h = h.to(device)\n",
    "\n",
    "    # get the output of the model\n",
    "    out, h = model(x, h)\n",
    "\n",
    "    # get the character probabilities\n",
    "    p = F.softmax(out, dim=1).data\n",
    "    p = p.cpu().numpy().squeeze()\n",
    "\n",
    "    # get top characters\n",
    "    if top_k is None:\n",
    "        top_ch = np.arange(vocab_size)\n",
    "    else:\n",
    "        p, top_ch = torch.topk(torch.tensor(p), top_k)\n",
    "        top_ch = top_ch.numpy().squeeze()\n",
    "        p = p.numpy().squeeze()\n",
    "\n",
    "    # select the likely next character with some randomness\n",
    "    char = np.random.choice(top_ch, p=p/p.sum())\n",
    "\n",
    "    return int_to_char[char], h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:53:25.514901Z",
     "start_time": "2025-05-18T12:53:25.509437Z"
    },
    "id": "qQ_MZBYblAG4"
   },
   "outputs": [],
   "source": [
    "# This function was adapted from Deep Learning by Prof. Paolo Favaro, University of Bern\n",
    "\n",
    "def generate(model, size, prime='The', top_k=None):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    # Prime the model with the initial characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = model.init_hidden(1)\n",
    "    if isinstance(h, tuple):\n",
    "        h = tuple(each.to(device) for each in h)\n",
    "    else:\n",
    "        h = h.to(device)\n",
    "\n",
    "    for ch in prime:\n",
    "        char, h = predict(model, ch, h, top_k=top_k, device=device)\n",
    "    chars.append(char)\n",
    "\n",
    "    # Generate characters\n",
    "    for _ in range(size):\n",
    "        char, h = predict(model, chars[-1], h, top_k=top_k, device=device)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5 **(2pts) Generate Text from Each Model.**\n",
    "- Use the `generate` function provided above to produce text from each of your models.\n",
    "- Generate 1000 characters of text.\n",
    "- Choose your own start word or prompt.\n",
    "- Experiment with different values of `top_k`\n",
    "- **Print** the generated text clearly for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-18T12:53:29.032531Z",
     "start_time": "2025-05-18T12:53:25.597475Z"
    },
    "id": "5MFzT3AIlAG8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elman text (top_k = 3):\n",
      "\n",
      " And mer with have my hours,\n",
      "What the wolld me would by the will the seaden shall be not this done.\n",
      "\n",
      "POMIONAE:\n",
      "That I am sharr here a manders of men oun and stands assell.\n",
      "\n",
      "CORIOLANUS:\n",
      "I'll sear me the shall he stare you meres theme the will and stand to but as thee,\n",
      "That, servingers there of him toor.\n",
      "\n",
      "CORIOLANUS:\n",
      "I here the raye heaven shall heart,\n",
      "And shal seat to ser will sorr heads the saul to strike the reare them, and, when I will serve yea hard\n",
      "As a manter to mar we come to mer\n",
      "And the with me hore that whene he shell here that\n",
      "Is manes the strench out off ame tell.\n",
      "\n",
      "CARISLA:\n",
      "That Is that what the senter treak,\n",
      "If the sarse a trought here to shee the striend to my son.\n",
      "\n",
      "CLIUS:\n",
      "And they so mer you are the rows off coursely sor,\n",
      "Theres men to the react, I have shall serve the say. I would bear hears the raye to the sheres the saules our crust a will\n",
      "I should shele sor that were you seaves\n",
      "And wish my sould, but to the will the shall so the warth the sare\n",
      "Till this it have son oun the\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Vanilla text (top_k = 3):\n",
      "\n",
      " And a sender the cause, sir they, and so have may so heaving the store\n",
      "The son them, thou art a parters. The stand an an and, and tell my hand,\n",
      "That he san she had the presert to make them\n",
      "If the such to this her hath han to her the son,\n",
      "And thou should thought assignt to heart a single hanger,\n",
      "A see his strenge oncest though a much of the wanther to me that thee\n",
      "To had the suntleman of him. How shalt that have so heart to\n",
      "great of heart and show have to make him here.\n",
      "\n",
      "LADY ANTH:\n",
      "The kink the persure a shall not, sir. What so to my heart of me so heavy sound the world strougd and,\n",
      "We should have, that have heart a peeputes the sunt and\n",
      "his shall have the come a sirr the crown her\n",
      "stand a peased them that to had the sunt and straithess of him to me,\n",
      "Which I have so may time that hath deeds of the wan on their honst the storment. When he shall now her thee.\n",
      "\n",
      "LEONTES:\n",
      "And he hath dies his presseds the sund the straith that well, as the wands offertless offection,\n",
      "When thou had my like her t\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Elman text (top_k = 5):\n",
      "\n",
      " And sore the ward and share of murts,\n",
      "And but tert we live an his faint on misely\n",
      "So mare my head themence, and my love as tay to dee herse to he wos the consuly.\n",
      "\n",
      "Privint:\n",
      "Who, so more alligentle sir.\n",
      "\n",
      "SIONGERST:\n",
      "That I' thene he shall where sire, I hould were you shear yoursed, and true me.\n",
      "\n",
      "CLIUCESTIO:\n",
      "Withress to mire all there this dadres the sout\n",
      "To have here this merred the elown, and we crouch than the reaturie them.\n",
      "They wear your string and to bean thy stean.\n",
      "\n",
      "LORIOLANUS:\n",
      "Not him thou as the chass of thee\n",
      "Weat that I, me wit harr thy founters, the stancouth that hound and treat as all hin a farse how hreen,\n",
      "This ingreste me have have.\n",
      "\n",
      "LERETES:\n",
      "Thou shisting ow mure her fain may hants and me that sorroush, with sir, then thou was day true,\n",
      "And shere her hast deed to that than she bay's how and shower, soo, a dorn thenely,\n",
      "And let madriget on sir for a mord on mune soust\n",
      "This it my san the risensty she wound we cherress.\n",
      "\n",
      "PRTIUS:\n",
      "Thou she this sound you.\n",
      "\n",
      "CLAUDIAN:\n",
      "A dreach out s\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Vanilla text (top_k = 5):\n",
      "\n",
      " And and shall nathe my first the crief the statest on this san shall the pott the wrich. I come of, and\n",
      "that shall to me the stome, me to that I say thee, sir, him are the partleman,\n",
      "The cupp a take the prect of the poor, and stores word.\n",
      "\n",
      "KING HESBRIV:\n",
      "Welter's leaves of man and mark thou seem this friend\n",
      "Take the boub them.\n",
      "\n",
      "LUVIO:\n",
      "Have you hast have beest how to me should she spend his pleave\n",
      "To seess the ware, but would he then whene thee and thoughted,\n",
      "He she had hellows the bust me that, see mentings, and his bound to have more as a man too must be duent of all to the word the prave make of her the seal to the course our cousud on and suppersed hath happy book.\n",
      "\n",
      "LEONTES:\n",
      "Wire they has mare all the procest and the winger to\n",
      "makes shorts art hel the pentleman, here treat, a sole ans to the\n",
      "cation, then I will the compitious,\n",
      "When he shall not this he with the crumber than he would thee\n",
      "And he would have have more the house, and where their fould stear made\n",
      "And the come to marriage the\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Elman text (top_k = 10):\n",
      "\n",
      " Ans mut frant dight;\n",
      "And when then we long mady day\n",
      "No han yeuse stand the charglle, sir, I have lood have\n",
      "Wis sain take, my againen age, will mineself I repel of hir plick; fur the wear.\n",
      "\n",
      "LORIO:\n",
      "I phall,\n",
      "I smen maven madren my shale; friel staingh he mout min\n",
      "As the deall a for ageineds, hore town the fosher and soor,\n",
      "Mart so have noner.\n",
      "\n",
      "CORINIA:\n",
      "Sir, secked and the drought the gull is the censer at her heart your praciof of tor that your grene, sor\n",
      "And and tooms if shall now, nothing hir chance is hear\n",
      "My meanes, that she trowh as I am the broves if meral of you?\n",
      "\n",
      "COMINGSEY:\n",
      "North sie, my ruchis to thy heare\n",
      "Herroth they benger thou day and thesers a frierd,\n",
      "To sace a greather trous indentrens.\n",
      "\n",
      "First\n",
      "TIO:\n",
      "I have all fire of here\n",
      "Weat souly brow now thou was the sell\n",
      "A as that wile ame tore\n",
      "The son and think of tuke of gore,\n",
      "Will sempse as here, we langlot wiel fave whece histed,\n",
      "And thith it illine hees well,\n",
      "And though and\n",
      "Marrous art offerst thes, tire,\n",
      "Whoth not then weran their st\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Vanilla text (top_k = 10):\n",
      "\n",
      " Ang your sword we sheel.\n",
      "\n",
      "PATRICHARD:\n",
      "I am all me. \n",
      "CISINIUS:\n",
      "They with cally meen!\n",
      "\n",
      "BENVOLIO:\n",
      "What you warth the hangert heaven bangs for the reskent to therefore.\n",
      "I have to be time, so cless of higher friends:\n",
      "As if you wasce to plase here he creak much dustife;\n",
      "How'lt, I'll the master. The hopes a muck befand: and lost age,\n",
      "Of his told from the for him a plece on the comment\n",
      "My lord, I know you was, my last to\n",
      "The befand's from him, bow my dott this\n",
      "come too and by my gentle walk is alone as heart;\n",
      "What shopld servist site, I have your prances and tuld honouraguty\n",
      "To take the bold be days hape crown's her,\n",
      "Will stoonged thou doible struegh's sweet\n",
      "Of the prayer that herself surpers, and be him hear;\n",
      "Inlight to mer stwearful withings wrong\n",
      "That her lords whose most mind hore hate: seepester as your consunds, the heart all of the death\n",
      "For take heaven thead, sir, the dain to him of that\n",
      "Trat the wullans is not to be death, but you have mind of my clangely, sereth to heart strike\n",
      "The him.\n",
      "\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "k_values=[3,5,10]\n",
    "\n",
    "for k in k_values:\n",
    "    text_elman = generate(model = Elman_RNN,size = 1000,prime = 'An',top_k = k)\n",
    "    print(f\"Elman text (top_k = {k}):\\n\\n\", text_elman)\n",
    "    print(\"\\n----------------------------------------------------------------------------------------------------------------\\n\")\n",
    "    text_vanilla = generate(model = Vanilla_RNN,size = 1000,prime = 'An',top_k = k)\n",
    "    print(f\"Vanilla text (top_k = {k}):\\n\\n\", text_vanilla)\n",
    "    print(\"\\n----------------------------------------------------------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6 **(3pts) Compare the output of the Elman RNN and PyTorch RNN.**\n",
    "In your answer, consider:\n",
    "- Fluency and resemblance to English\n",
    "- Word-like structure or gibberish\n",
    "- The effect of `top_k` on randomness vs. coherence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6x4vzbX2lAG-"
   },
   "source": [
    "Fluency/ Looks like english:\n",
    "   - Both the custom Elman and Pytorch models have low phrase coherence/syntax and low understandable meaning. (working on char-level approximation, not words/phrases).\n",
    "   - Pytorch has more recognizable phrases or parts of phrases, whereas the custom Elman looks more like old / archaic English.\n",
    "\n",
    "Word-like structure:\n",
    "   - Custom Elman produces more archaic/gibberish words, words with incorrect spellings (e.g. too many/not enough letters or the wrong order of letters in a word) than PyTorch, which produces better spellings of words.\n",
    "\n",
    "Effect of top_k :\n",
    "   - Larger k values allow for more random words and spellings, as well as a greater variety of phrases and more 'creativity' in wording for both models. However, there is still not much of a real coherence/syntax in phrases.\n",
    "   - Low k values allow for more words to resemble to real English and achieve some coherence/syntax in phrases. However, there is maybe more repetition of similar words/wording, phrase structures/syntax (probably due to higher existing frequencies in the English text used as data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97KmLu7me-tg"
   },
   "source": [
    "# 4. Code Comprehension (6 points)\n",
    "Before wrapping up, letâ€™s test your understanding of the character-level RNN code you just explored. For each topic, identify which statements are TRUE âœ… and which are FALSE âŒ. Each question may have 1, 2, 3, or 4 correct answers.\n",
    "\n",
    "- 4 correct answers: 2 points\n",
    "- 3 correct answers: 1 point\n",
    "- 2 or fewer correct answers: 0 points\n",
    "\n",
    "ðŸ’¡ In Google Colab, you can easily add emojis to markdown cells by typing `:` followed by the emoji's name. For example, typing `:light-bulb` will display a light bulb emoji. This feature is also available as an extension in many IDEs.\n",
    "\n",
    "â— **TIP:** If a term is unfamiliar to you, look it up in [Google's ML Glossary](https://developers.google.com/machine-learning/glossary) for a simple explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-O9Y6SLe-th"
   },
   "source": [
    "#### 4.1 **(2pts) Which of the following statements describe characteristics of Elmanâ€™s RNN in the context of character-level text generation?**\n",
    "\n",
    " A. The hidden state $h_t$ m is computed using only the current character input $x_t$. âŒ\n",
    "\n",
    " B. The RNN reads one character at a time and updates its hidden state sequentially. âœ…\n",
    "\n",
    " C. The model uses convolutional layers to capture character patterns. âŒ\n",
    "\n",
    " D. The output at each timestep predicts the probability of the next character. âœ…\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BKIr-taHe-th"
   },
   "source": [
    "#### 4.2 **(2pts) What components are essential in character-level RNNs for training on a corpus of text?**\n",
    "\n",
    " A. A vocabulary of all possible characters âœ…\n",
    "\n",
    " B. One-hot or embedding representation of characters âœ…\n",
    "\n",
    " C. A loop or recurrence that updates the hidden state per character âœ…\n",
    "\n",
    " D. A decoder that transforms characters to words âŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX2vvVdue-th"
   },
   "source": [
    "#### 4.3 **(2pts) Which of the following statements correctly describe the role and effect of the `top_k` parameter in character-level text generation?**\n",
    "\n",
    " A. `top_k` directly affects the training process and gradient computation. âŒ\n",
    "\n",
    " B. Decreasing `top_k` leads to more diverse but potentially less coherent text. âŒ\n",
    "\n",
    " C. Setting `top_k` to 1 is equivalent to greedy sampling (always choosing the most likely character). âœ…\n",
    "\n",
    " D. `top_k` ensures syntactic correctness by sampling from grammatically valid continuations. âŒ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3yzkn9pAWN4v"
   },
   "source": [
    "---\n",
    "Great work reaching the end of this assignment! ðŸŒŸ\n",
    "\n",
    "Youâ€™ve implemented your own Elman RNN, trained multiple models to generate text, and compared their outputs in terms of fluency and structure. Along the way, you explored how sampling strategies like `top_k` influence creativity vs. coherence in generated sequences.\n",
    "\n",
    "Although these character-level generative models canâ€™t yet write Shakespearean plays, youâ€™ve taken a big step toward understanding how machines learn to generate language. Keep experimenting, try new corpora, tweak hyperparameters, or explore word-level generation next! ðŸš€âœ¨"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1qtKzLvb2sMnivcmAX7vDZNUjkDTW8BCs",
     "timestamp": 1742751475201
    },
    {
     "file_id": "1lHTdcnKRobA8pkiahgabEhno0KIHBYXH",
     "timestamp": 1742720380363
    },
    {
     "file_id": "1hJOyTwwI9H7KCEurp6K9JBTvsXD0sDv-",
     "timestamp": 1742051729167
    },
    {
     "file_id": "1kF72q-F9PJULXcS2ufdDPgIGc_q1l9Lo",
     "timestamp": 1741887953352
    }
   ]
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
